{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d267d9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing ChromaDB at C:\\Users\\hari7\\Documents\\Anokha Hackthon\\ChromaDB\n",
      "\n",
      "Phase 2: Testing Generalization...\n",
      "\n",
      "Question: Who am I?\n",
      "Retrieved Context: ['My name is Hari.', 'My name is Hari.']\n",
      "AI Answer: You're Hari.\n",
      "\n",
      "Question: Do I know how to code?\n",
      "Retrieved Context: ['I am an expert in Python and Flask.', 'I am an expert in Python and Flask.']\n",
      "AI Answer: You're proficient in coding, specifically Python and Flask.\n",
      "\n",
      "Question: Where do I live?\n",
      "Retrieved Context: ['I live in the southern part of India.', 'I live in the southern part of India.']\n",
      "AI Answer: You live in the southern part of India.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "import ollama\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "DB_PATH = r\"C:\\Users\\hari7\\Documents\\Anokha Hackthon\\ChromaDB\"\n",
    "\n",
    "# 1. Setup Database\n",
    "print(f\"Initializing ChromaDB at {DB_PATH}\")\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "collection = client.get_or_create_collection(name=\"user_test_facts\")\n",
    "\n",
    "# 2. Helper Functions\n",
    "\n",
    "def save_fact(fact):\n",
    "    print(f\"Saving: '{fact}'\")\n",
    "    collection.add(\n",
    "        documents=[fact],\n",
    "        ids=[str(uuid.uuid4())]\n",
    "    )\n",
    "\n",
    "def query_and_answer(user_question):\n",
    "    print(f\"\\nQuestion: {user_question}\")\n",
    "    \n",
    "    # A. Retrieve\n",
    "    results = collection.query(\n",
    "        query_texts=[user_question],\n",
    "        n_results=2 \n",
    "    )\n",
    "    retrieved_facts = results['documents'][0]\n",
    "    \n",
    "    print(f\"Retrieved Context: {retrieved_facts}\")\n",
    "\n",
    "    if not retrieved_facts:\n",
    "        print(\"No relevant memory found.\")\n",
    "        return\n",
    "\n",
    "    # --- THE GENERALIZED PROMPT ---\n",
    "    # This prompt works for ANY user and ANY context.\n",
    "    prompt = f\"\"\"\n",
    "    You are a helpful AI Assistant with long-term memory.\n",
    "    \n",
    "    =========================================\n",
    "    MEMORY BLOCK (Past statements from the User):\n",
    "    {retrieved_facts}\n",
    "    =========================================\n",
    "    \n",
    "    INSTRUCTIONS:\n",
    "    1. The 'MEMORY BLOCK' contains facts the user told you previously. \n",
    "    2. If the memory says \"I am X\" or \"I like Y\", it refers to the USER (the person you are talking to).\n",
    "    3. Use this memory to answer the user's current question accurately.\n",
    "    4. Speak naturally. Do not say \"The memory says...\". Just answer as if you know it.\n",
    "    \n",
    "    USER QUESTION: {user_question}\n",
    "    \n",
    "    ANSWER:\n",
    "    \"\"\"\n",
    "    \n",
    "    # C. Generate\n",
    "    response = ollama.chat(model=MODEL_NAME, messages=[{'role': 'user', 'content': prompt}])\n",
    "    print(f\"AI Answer: {response['message']['content']}\")\n",
    "\n",
    "# 3. Test Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Only run the teaching phase if you want to add NEW facts. \n",
    "    # Since you already ran it once, the data is saved!\n",
    "    # You can comment out the 'facts' loop if you don't want duplicate data.\n",
    "    \n",
    "    # facts = [\n",
    "    #     \"My name is Hari.\",\n",
    "    #     \"I am an expert in Python and Flask.\",\n",
    "    #     \"I prefer working in startups rather than MNCs.\",\n",
    "    #     \"I live in the southern part of India.\"\n",
    "    # ]\n",
    "    # for f in facts:\n",
    "    #     save_fact(f)\n",
    "\n",
    "    print(\"\\nPhase 2: Testing Generalization...\")\n",
    "\n",
    "    # Test 1: Identity\n",
    "    query_and_answer(\"Who am I?\")\n",
    "\n",
    "    # Test 2: Inference\n",
    "    query_and_answer(\"Do I know how to code?\")\n",
    "\n",
    "    # Test 3: Location\n",
    "    query_and_answer(\"Where do I live?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22b73077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STEP 1: Teaching the AI your Profile\n",
      "Saving User Fact: 'My name is Hari.'\n",
      "Saving User Fact: 'I am proficient in Python and MATLAB.'\n",
      "Saving User Fact: 'I have worked on projects involving Signal Processing and EEG data analysis.'\n",
      "Saving User Fact: 'I am interested in the medical or healthcare technology field.'\n",
      "\n",
      "STEP 2: The Real-Life Test\n",
      "\n",
      "User asks: \"What kind of internships should I apply for?\"\n",
      "AI Remembers: ['I am interested in the medical or healthcare technology field.', 'I am proficient in Python and MATLAB.', 'I have worked on projects involving Signal Processing and EEG data analysis.']\n",
      "\n",
      "AI Reply: Based on your interest in medical or healthcare technology and your proficiency in Python and MATLAB, along with experience in Signal Processing and EEG data analysis, here are some internship suggestions tailored to your skills:\n",
      "\n",
      "1. **Signal Processing Internship**: Apply for internships that involve working with signal processing algorithms to analyze biomedical signals. This could include companies or research institutions focusing on medical imaging, audio signal processing, or other related fields.\n",
      "\n",
      "2. **Neurotechnology Internship**: Your experience in EEG data analysis makes you a strong candidate for neurotechnology-focused internships. Companies and startups working on brain-computer interfaces (BCIs), neural prosthetics, or neurostimulation devices might be an excellent fit.\n",
      "\n",
      "3. **Medical Imaging Analysis Internship**: Given your proficiency in MATLAB and Python, you could consider internships that involve medical imaging analysis software development. This area is crucial for applications such as image segmentation, registration, or feature extraction in MRI and CT scans.\n",
      "\n",
      "4. **Healthcare Technology Development Internship**: With a background in both programming languages (Python and MATLAB) and practical experience in data analysis (EEG), you might find opportunities in healthcare technology companies working on wearable devices, telemedicine platforms, or electronic health records systems.\n",
      "\n",
      "5. **Research Assistant Position in Biomedical Engineering**: If your interest is more towards research, look for positions as a research assistant in biomedical engineering departments of universities that focus on signal processing, neuroengineering, and medical imaging. This will give you hands-on experience with projects related to healthcare technology.\n",
      "\n",
      "These suggestions aim to leverage your unique combination of skills and interests, increasing the likelihood of securing an internship that aligns closely with your career goals.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "import ollama\n",
    "\n",
    "# Configuration\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "DB_PATH = r\"C:\\Users\\hari7\\Documents\\Anokha Hackthon\\ChromaDB\"\n",
    "\n",
    "# Initialize Client\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "\n",
    "# Clean Start\n",
    "try:\n",
    "    client.delete_collection(\"user_test_facts\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"user_test_facts\")\n",
    "\n",
    "# Helper Functions\n",
    "\n",
    "def save_fact(fact):\n",
    "    print(f\"Saving User Fact: '{fact}'\")\n",
    "    collection.add(\n",
    "        documents=[fact],\n",
    "        ids=[str(uuid.uuid4())]\n",
    "    )\n",
    "\n",
    "def ask_agent(question):\n",
    "    print(f\"\\nUser asks: \\\"{question}\\\"\")\n",
    "    \n",
    "    # Retrieve\n",
    "    results = collection.query(\n",
    "        query_texts=[question], \n",
    "        n_results=3 \n",
    "    )\n",
    "    retrieved_memory = results['documents'][0]\n",
    "    \n",
    "    print(f\"AI Remembers: {retrieved_memory}\")\n",
    "    \n",
    "    # Generate\n",
    "    prompt = f\"\"\"\n",
    "    You are a Career Counselor.\n",
    "    \n",
    "    MEMORY OF USER (What they told you before):\n",
    "    {retrieved_memory}\n",
    "    \n",
    "    USER QUESTION: \"{question}\"\n",
    "    \n",
    "    INSTRUCTION:\n",
    "    Answer the question based strictly on the User's MEMORY. \n",
    "    Do not give generic advice. Tailor the specific job titles to their skills found in memory.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(model=MODEL_NAME, messages=[{'role': 'user', 'content': prompt}])\n",
    "    print(f\"\\nAI Reply: {response['message']['content']}\")\n",
    "\n",
    "# Execution\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"STEP 1: Teaching the AI your Profile\")\n",
    "    \n",
    "    save_fact(\"My name is Hari.\")\n",
    "    save_fact(\"I am proficient in Python and MATLAB.\")\n",
    "    save_fact(\"I have worked on projects involving Signal Processing and EEG data analysis.\")\n",
    "    save_fact(\"I am interested in the medical or healthcare technology field.\")\n",
    "\n",
    "    print(\"\\nSTEP 2: The Real-Life Test\")\n",
    "    \n",
    "    ask_agent(\"What kind of internships should I apply for?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00883c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User says: Hi, I am Hari and I am learning AI.\n",
      "Auto-Extracting info: The fact extracted from the user text is:\n",
      "Hari is learning AI.\n",
      "Saved to Memory automatically.\n",
      "\n",
      "User says: Can you tell me a joke?\n",
      "No new facts detected in this message.\n",
      "\n",
      "User says: I recently started working with Docker containers.\n",
      "Auto-Extracting info: The extracted fact from the user text is:\n",
      "\n",
      "\"I recently started working with Docker containers.\"\n",
      "\n",
      "However, since this is a statement about an activity they've been doing, it's not necessarily a factual info about themselves. A more neutral extraction would be:\n",
      "\n",
      "\"Has experience with Docker containers.\" \n",
      "\n",
      "But in the format requested, I'll keep the original: \n",
      "\n",
      "I recently started working with Docker containers.\n",
      "Saved to Memory automatically.\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "import uuid\n",
    "import ollama\n",
    "\n",
    "MODEL_NAME = \"llama3.1:8b\"\n",
    "DB_PATH = r\"C:\\Users\\hari7\\Documents\\Anokha Hackthon\\ChromaDB\"\n",
    "\n",
    "client = chromadb.PersistentClient(path=DB_PATH)\n",
    "collection = client.get_or_create_collection(name=\"user_test_facts\")\n",
    "\n",
    "def auto_save_memory(user_input):\n",
    "    \"\"\"\n",
    "    This function uses the LLM to decide IF something should be saved.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Ask LLM to extract facts\n",
    "    extraction_prompt = f\"\"\"\n",
    "    Analyze the following user text. \n",
    "    If the user mentions a fact about themselves (Name, Skills, Experience, Goals), extract it as a short sentence.\n",
    "    If there is no factual info (e.g., \"Hello\", \"Thank you\"), return exactly \"NO_FACT\".\n",
    "    \n",
    "    User Text: \"{user_input}\"\n",
    "    \"\"\"\n",
    "    \n",
    "    response = ollama.chat(model=MODEL_NAME, messages=[{'role': 'user', 'content': extraction_prompt}])\n",
    "    extracted_data = response['message']['content'].strip()\n",
    "    \n",
    "    # 2. Logic to Save\n",
    "    if \"NO_FACT\" not in extracted_data:\n",
    "        print(f\"Auto-Extracting info: {extracted_data}\")\n",
    "        collection.add(\n",
    "            documents=[extracted_data],\n",
    "            ids=[str(uuid.uuid4())]\n",
    "        )\n",
    "        print(\"Saved to Memory automatically.\")\n",
    "    else:\n",
    "        print(\"No new facts detected in this message.\")\n",
    "\n",
    "# --- Simulation ---\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # Scenario 1: User introduces themselves (LLM should detect this)\n",
    "    user_message_1 = \"Hi, I am Hari and I am learning AI.\"\n",
    "    print(f\"\\nUser says: {user_message_1}\")\n",
    "    auto_save_memory(user_message_1)\n",
    "    \n",
    "    # Scenario 2: User says something random (LLM should ignore this)\n",
    "    user_message_2 = \"Can you tell me a joke?\"\n",
    "    print(f\"\\nUser says: {user_message_2}\")\n",
    "    auto_save_memory(user_message_2)\n",
    "    \n",
    "    # Scenario 3: User adds a skill (LLM should detect this)\n",
    "    user_message_3 = \"I recently started working with Docker containers.\"\n",
    "    print(f\"\\nUser says: {user_message_3}\")\n",
    "    auto_save_memory(user_message_3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
